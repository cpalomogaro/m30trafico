notamediaCluster1 <- subset(notamediaCluster, notamediaCluster$Group.1==1)
notamediaCluster1
usuario5 <- subset(notas, notas$userID==5)
usuario5
notamediaCluster8 <- subset(notamediaCluster, notamediaCluster$Group.1==8)
notamediaCluster8
usuario405 <- subset(notas, notas$userID==405)
usuario405
library(softImpute)
ratingmat <- dcast(rating, userID~movieID, value.var = "punctuation", na.rm=FALSE)
usuario5mat <- subset(ratingmat, ratingmat$userID==5)
ratingmat <- ratingmat[-c(5),]
# imputation <- softImpute(ratingmat, rank.max = 2, trace.it = FALSE)
library(data.table)
set.seed(1234)
movies <- read.table('/home/cris/Documentos/Experto Data Science/10.SistemasRecomendacion/Practica/dat/ml-100k/u.item', header=F, sep='|', quote='"')
movies <- movies[, -c(3:5)]
colnames(movies) <- c("movieID", "title", "Unknown", "Action", "Adventure", "Animation", "Childrens", "Comedy", "Crime", "Documentary", "Drama", "Fantasy", "FilmNoir", "Horror", "Musical", "Mystery", "Romance", "SciFi", "Thriller", "War", "Western")
rating <- read.table('/home/cris/Documentos/Experto Data Science/10.SistemasRecomendacion/Practica/dat/ml-100k/u.data', header=F, sep='', quote='"')
rating <- rating[, -c(4)]
setnames(rating, c('userID', 'movieID', 'punctuation'))
users <- read.table('/home/cris/Documentos/Experto Data Science/10.SistemasRecomendacion/Practica/dat/ml-100k/u.user', header=F, sep='|', quote='"')
users <- users[, -c(5)]
setnames(users, c('userID', 'age', 'gender', 'occupation'))
# creo una tabla con toda la información
total <- merge(rating, movies, by='movieID')
total <- merge(total, users, by='userID')
total02 <- total[['punctuation']]*total[-c(1,2, 3, 4, 24, 25, 26)]
#uno esta tabla a la anterior, borrando los géneros
total01 <- total[, -c(5:23)]
notaGenero <- cbind(total01, total02)
notaGenero[notaGenero == 0] <- NA
#nota media de cada usuario a los géneros
notas <- aggregate(notaGenero[, 8:26], list(notaGenero$userID), mean, na.rm=TRUE)
setnames(notas, c('userID', "Unknown", "Action", "Adventure", "Animation", "Childrens", "Comedy", "Crime", "Documentary", "Drama", "Fantasy", "FilmNoir", "Horror", "Musical", "Mystery", "Romance", "SciFi", "Thriller", "War", "Western"))
#Clasificamos en 10 clústers
distance <- dist(notas[2:20], method='euclidean')
clusterUsers <-  hclust(distance, method='ward.D')
numCluster <- cutree(clusterUsers, k=10)
clusters <- as.data.frame(numCluster)
usersCluster<- data.frame(cbind(users, clusters))
usersCluster <- usersCluster[, -c(2:4)]
library(softImpute)
library(softImpute)
library(reshape2)
library(plyr)
library(NMF)
library(ggplot2)
set.seed(1234)
pelis <- read.table('/home/cris/Documentos/Experto Data Science/10.SistemasRecomendacion/Practica/dat/ml-100k/u.data', sep = "\t", quote = "")
generos <- read.table('/home/cris/Documentos/Experto Data Science/10.SistemasRecomendacion/Practica/dat/ml-100k/u.item', sep = "|", quote = "")
colnames(pelis) <- c("user", "movieid", "rating", "date")
colnames(generos) <- c("movieid",
"movietitle",
"releasedate",
"videoreleasedate",
"IMDbURL",
"unknown",
"Action",
"Adventure",
"Animation",
"Children",
"Comedy",
"Crime",
"Documentary",
"Drama",
"Fantasy",
"Film-Noir",
"Horror",
"Musical",
"Mystery",
"Romance",
"Sci-Fi",
"Thriller",
"War",
"Western")
tmp <- merge(pelis, generos)
tmp$movietitle <- tmp$releasedate <- tmp$videoreleasedate <- tmp$IMDbURL <- tmp$date <- NULL
tmp <- melt(tmp, id.vars = c("user", "movieid", "rating"))
tmp <- tmp[tmp$value == 1,]
tmp <- ddply(tmp, .(user, variable), summarize, rating = mean(rating))
tmp <- dcast(tmp, user ~ variable)
m   <- as.matrix(tmp[,-1])
rownames(m) <- tmp[,1]
m[is.na(m)] <- 0
res <- nmf(m, 6)
h <- res@fit@H
#h[h < 0.00001] <- 0
h <- 100 * h / rowSums(h)
h <- data.frame(h)
h$factor <- 1:nrow(h)
h <- melt(h, id.vars = "factor")
ggplot(h, aes(x = variable, y = value)) + geom_bar(stat = "identity") + facet_grid(factor~.)
res02 <- nmf(m, 10)
h02 <- res02@fit@H
#h[h < 0.00001] <- 0
h02 <- 100 * h02 / rowSums(h02)
h02 <- data.frame(h02)
h02$factor <- 1:nrow(h02)
h02 <- melt(h02, id.vars = "factor")
ggplot(h02, aes(x = variable, y = value)) + geom_bar(stat = "identity") + facet_grid(factor~.)
knitr::opts_chunk$set(echo = TRUE)
source("../funciones/generaGraficosDinamicos.R")
dat <- readRDS(params$res_file)
library(dplyr)
library(data.table)
library(ggplot2)
library(xts)
library(dygraphs)
library(reshape2)
library(ggmap)
library(knitr)
library(OIsurv)
library(survival)
library(survminer)
library(ranger)
library(ggmap)
## Carga de los datasets: (Puede verse el proceso de limpieza de los datasets en ../src/obtencionInformacion.R)
recibidos <- readRDS('../res/recibidos.rds')
resueltos <- readRDS('../res/resueltos.rds')
sinresolver <- readRDS('../res/sinresolver.rds')
recibidos <- recibidos %>%
mutate(FECHA_DE_RECEPCION = as.Date(FECHA_DE_RECEPCION, format= '%Y-%m-%d'))
resueltos <- resueltos %>%
mutate(FECHA_DE_RECEPCION = as.Date(FECHA_DE_RECEPCION, format = '%Y-%m-%d'),
FECHA_DE_RESOLUCION = as.Date(FECHA_DE_RESOLUCION, format = '%Y-%m-%d'))
sinresolver <- sinresolver %>%
mutate(FECHA_DE_RECEPCION = as.Date(FECHA_DE_RECEPCION, format= '%Y-%m-%d'))
ID_anomalia <- read.csv('../res/ID_anomalia.csv', sep = ',', header = TRUE, fileEncoding = 'latin1')
ID_seccion <- read.csv('../res/ID_seccion.csv', sep = ',', header = TRUE , fileEncoding = 'latin1')
ID_tipoincidencia <- read.csv('../res/ID_tipoincidencia.csv', sep = ',', header = TRUE, fileEncoding = 'latin1')
ID_barrio <- read.csv('../res/ID_barrio.csv', sep = ',', header = TRUE, fileEncoding = 'latin1')
ID_canal <- read.csv('../res/ID_canal.csv', sep = ',', header = TRUE, fileEncoding = 'latin1')
ID_distrito <- read.csv('../res/ID_distrito.csv', sep = ',', header = TRUE, fileEncoding = 'latin1')
ID_organismo1 <- read.csv('../res/ID_Organismo1.csv', sep = ',', header = TRUE, fileEncoding = 'latin1')
ID_organismo2 <- read.csv('../res/ID_Organismo2.csv', sep = ',', header = TRUE, fileEncoding = 'latin1')
ID_organismo3 <- read.csv('../res/ID_Organismo3.csv', sep = ',', header = TRUE, fileEncoding = 'latin1')
tmp <- recibidos %>%
select(FECHA_DE_RECEPCION) %>%
mutate(FECHA_DE_RECEPCION = as.Date(FECHA_DE_RECEPCION), format= '%yyyy/%MM/%dd') %>%
group_by(FECHA_DE_RECEPCION) %>%
count()
colnames(tmp)[colnames(tmp) == 'n'] <- 'numero_de_avisos'
tmp02 <- tmp %>%
select(FECHA_DE_RECEPCION, numero_de_avisos) %>%
mutate(ano = format(FECHA_DE_RECEPCION, '%Y')) %>%
group_by(ano) %>%
summarise(numero_de_avisos=sum(numero_de_avisos))
kable(tmp02, col.names = c('año', 'número de avisos'), format = 'markdown')
tmp <- xts(tmp, order.by = tmp$FECHA_DE_RECEPCION)
p1 <- dygraph(tmp) %>%
dyAxis("y", label = "Número de avisos") %>%
dyLegend(show = "onmouseover", hideOnMouseOut = TRUE, labelsSeparateLines = TRUE) %>%
dyRangeSelector()
p1
tmp <- recibidos %>%
select(CANAL_DE_ENTRADA_ID) %>%
group_by(CANAL_DE_ENTRADA_ID) %>%
count()
colnames(tmp)[colnames(tmp) == 'n'] <- 'numero_de_avisos'
tmp <- merge(tmp, ID_canal, by= 'CANAL_DE_ENTRADA_ID')
p2 <- ggplot(tmp, aes(x = CANAL_DE_ENTRADA, y = numero_de_avisos, fill= CANAL_DE_ENTRADA)) +
geom_bar(stat = 'identity') + scale_fill_brewer(palette = "Blues") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  guides(fill=FALSE)
p2
tmp <- recibidos %>%
select(CANAL_DE_ENTRADA_ID, HORA_DE_RECEPCION)
tmp$HORA_DE_RECEPCION <- strptime(tmp$HORA_DE_RECEPCION, format = '%H:%M:%S')
tmp$cut15 <- cut(tmp$HORA_DE_RECEPCION, breaks = "15 min")
tmp <- tmp %>%
select(CANAL_DE_ENTRADA_ID, cut15) %>%
group_by(CANAL_DE_ENTRADA_ID, cut15) %>%
count()
tmp <- merge(tmp, ID_canal, by = 'CANAL_DE_ENTRADA_ID')
tmp$Hora <- format(strptime(tmp$cut15, format = '%Y-%m-%d %H:%M:%S'), '%H:%M')
colnames(tmp)[colnames(tmp) == 'n'] <- 'numero_de_avisos'
p3 <- ggplot(tmp, aes(x = Hora, y = numero_de_avisos, group = CANAL_DE_ENTRADA)) + geom_line(aes(color=CANAL_DE_ENTRADA)) + scale_x_discrete(breaks=c("00:00","04:00", "08:00","12:00", "16:00", "20:00", "23:45"))
p3
tmp <- recibidos %>%
group_by(SECCION_ID) %>%
count()
tmp <- merge(tmp, ID_seccion, by = 'SECCION_ID')
tmp <- tmp %>% top_n(11, wt=n)
g1 <- ggplot(tmp, aes(x = reorder(SECCION, -n), y = n, fill= reorder(SECCION, -n))) +
geom_bar(stat = 'identity') + theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
guides(fill=FALSE)
g1
tmp <- recibidos %>%
select(FECHA_DE_RECEPCION, SECCION_ID) %>%
mutate(FECHA_DE_RECEPCION = format(FECHA_DE_RECEPCION, '%Y-%m-%d')) %>%
filter(SECCION_ID == 2 | SECCION_ID == 5 | SECCION_ID == 22 | SECCION_ID == 33 |
SECCION_ID == 34 | SECCION_ID == 41 | SECCION_ID == 42 | SECCION_ID == 46 |
SECCION_ID == 57 | SECCION_ID == 59 | SECCION_ID == 65) %>%
group_by(FECHA_DE_RECEPCION, SECCION_ID) %>%
count()
tmp <- merge(tmp, ID_seccion, by = 'SECCION_ID')
graficas <- function(seccion){
tmp <- tmp %>%
mutate(FECHA_DE_RECEPCION = as.Date(FECHA_DE_RECEPCION, '%Y-%m-%d')) %>%
filter(SECCION_ID == seccion)
g <- ggplot(tmp, aes(x = FECHA_DE_RECEPCION, y = n, group= SECCION)) + geom_line() + labs(title=tmp$SECCION,
x ="Fecha", y = "Número de avisos/día") + theme(plot.title = element_text(size=6)) + scale_x_date()
print(g)
}
par(mfrow = c(2, 2))
graficas(2)
graficas(22)
graficas(5)
graficas(33)
graficas(34)
graficas(41)
graficas(42)
graficas(46)
graficas(57)
graficas(59)
graficas(65)
tmp <- recibidos %>%
select(FECHA_DE_RECEPCION, SECCION_ID) %>%
mutate(FECHA_DE_RECEPCION = format(FECHA_DE_RECEPCION, '%Y-%m-%d')) %>%
filter(SECCION_ID == 2 | SECCION_ID == 5 | SECCION_ID == 22 | SECCION_ID == 33 |
SECCION_ID == 34 | SECCION_ID == 41 | SECCION_ID == 42 | SECCION_ID == 46 |
SECCION_ID == 57 | SECCION_ID == 59 | SECCION_ID == 65) %>%
group_by(FECHA_DE_RECEPCION, SECCION_ID) %>%
count()
tmp <- merge(tmp, ID_seccion, by = 'SECCION_ID')
graficas <- function(seccion){
tmp <- tmp %>%
mutate(FECHA_DE_RECEPCION = as.Date(FECHA_DE_RECEPCION, '%Y-%m-%d')) %>%
filter(SECCION_ID == seccion)
g <- ggplot(tmp, aes(x = FECHA_DE_RECEPCION, y = n, group= SECCION)) + geom_line() + labs(title=tmp$SECCION,
x ="Fecha", y = "Número de avisos/día") + theme(plot.title = element_text(size=6)) + scale_x_date()
print(g)
}
par(mfrow = c(2, 2))
graficas(2)
graficas(22)
graficas(5)
graficas(33)
graficas(34)
graficas(41)
graficas(42)
graficas(46)
graficas(57)
graficas(59)
graficas(65)
# El proceso para obtener las localizaciones es algo largo, por lo que el código se puede consultar en '../src/BiciMad.R'. Al final guardo el resultado en un csv que leo a continuación:
tmp <- read.csv('../res/latlon_bicirobada.csv', header = TRUE, fileEncoding = 'latin1')
madrid <- c(lon = -3.702234, lat = 40.48) #coordenadas para madrid
madrid_map <- get_map(madrid, maptype = 'roadmap', zoom = 11)
ggmap(madrid_map, extent = 'device') +
geom_point(data = tmp, aes(x = lon, y = lat, size = sqrt(tmp$num_veces)), alpha = .2)
madrid02 <- c(lon = -3.702234, lat = 40.4835) #coordenadas para madrid
madrid_map02 <- get_map(madrid02, maptype = 'roadmap', zoom = 13)
ggmap(madrid_map02, extent = 'device') +
geom_point(data = tmp, aes(x = lon, y = lat, size = sqrt(tmp$num_veces)), alpha = .2)
madrid03 <- c(lon = -3.702234, lat = 40.4) #coordenadas para madrid
madrid_map03 <- get_map(madrid03, maptype = 'roadmap', zoom = 13)
ggmap(madrid_map03, extent = 'device') +
geom_point(data = tmp, aes(x = lon, y = lat, size = sqrt(tmp$num_veces)), alpha = .2)
recibidos02 <- recibidos %>%
select(FECHA_DE_RECEPCION, SECCION_ID, ANOMALIA_ID) %>%
group_by(FECHA_DE_RECEPCION) %>%
count()
resueltos02 <- resueltos %>%
select(FECHA_DE_RECEPCION, SECCION_ID, ANOMALIA_ID) %>%
group_by(FECHA_DE_RECEPCION) %>%
count()
recibidos02$FECHA_DE_RECEPCION <- as.Date(recibidos02$FECHA_DE_RECEPCION, format = "%Y-%m-%d")
recibidos02 <- recibidos02[order(recibidos02$FECHA_DE_RECEPCION),]
recibidos_xts <- xts(recibidos02, order.by = recibidos02$FECHA_DE_RECEPCION)
resueltos02$FECHA_DE_RECEPCION <- as.Date(resueltos02$FECHA_DE_RECEPCION, format = "%Y-%m-%d")
resueltos02 <- resueltos02[order(resueltos02$FECHA_DE_RECEPCION),]
resueltos_xts <- xts(resueltos02, order.by = resueltos02$FECHA_DE_RECEPCION)
ambos <- cbind(recibidos_xts, resueltos_xts)
ambos <- as.data.frame(ambos)
ambos$n <- as.numeric(ambos$n)
ambos$n.1 <- as.numeric(ambos$n.1)
ambos[is.na(ambos)] <- 0
ambos$pendientes <- cumsum(ambos$n.1 - ambos$n)
ambos$FECHA_DE_RECEPCION.1 <- as.Date(ambos$FECHA_DE_RECEPCION.1, format = '%Y-%m-%d')
tmp <- xts(ambos$pendientes, order.by = ambos$FECHA_DE_RECEPCION.1)
plot(tmp, main = "Avisos pendientes de resolución" ,
ylab = "cola de pendientes")
tmp <- sinresolver %>%
select(SECCION_ID) %>%
group_by(SECCION_ID) %>%
count()
tmp <- merge(tmp, ID_seccion, by = 'SECCION_ID')
tmp <- tmp %>% top_n(11, wt=n)
g1 <- ggplot(tmp, aes(x = reorder(SECCION, -n), y = n, fill= reorder(SECCION, -n))) +
geom_bar(stat = 'identity') + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
guides(fill=FALSE)
g1
# El proceso está detallado en '../res/supervivencia.R'
resueltos$COORDENADA_OFICIAL_X <- resueltos$COORDENADA_OFICIAL_Y <- resueltos$FECHA_DE_RECEPCION <- resueltos$FECHA_DE_RESOLUCION <- NULL
resueltos$weekdayResolucion <- resueltos$HORA_DE_RECEPCION <- resueltos$HORA_DE_RESOLUCION <- resueltos$direccion <- NULL
sinresolver$COORDENADA_OFICIAL_X <- sinresolver$COORDENADA_OFICIAL_Y <- sinresolver$FECHA_DE_RECEPCION <- sinresolver$direccion <- sinresolver$HORA_DE_RECEPCION <- NULL
resueltos$solucionado <- 1
sinresolver$solucionado <- 0
colnames(sinresolver)[colnames(sinresolver) == 'tiempo_sin_resolver'] <- 'tiempo'
colnames(resueltos)[colnames(resueltos) == 'tiempo_de_resolucion'] <- 'tiempo'
total <- rbind(resueltos, sinresolver)
total <- merge(total, ID_seccion, by= 'SECCION_ID')
total$tiempo <- as.numeric(total$tiempo)
total <- total[!is.na(total$tiempo),]
#Por problemas de memoria, escojo una muestra aleatoria de 5000 entradas
set.seed(1234)
tmp <- sample_n(total, 5000)
kaplan <- Surv(tmp$tiempo, tmp$solucionado)
fit_kaplan <- survfit(kaplan ~ 1)
ggsurvplot(fit_kaplan, main = 'Kaplan Meyer Plot') + labs(
x = "Tiempo (días)",
y    = "Probabilidad de no ser resuelta"
)
fit02 <- survfit(Surv(tiempo, solucionado) ~ SECCION, data=tmp)
ggsurvplot(fit02, pval=TRUE,
main="Kaplan-Meier para tiempo de resolucion") + labs(
x = "Tiempo (días)",
y    = "Probabilidad de no ser resuelta"
)
cox <- coxph(Surv(tiempo, solucionado) ~ SECCION, data=tmp)
summary(cox)
cox_fit <- survfit(cox)
estado.trafico.zgz <- readLines("https://opendata.aemet.es/opendata/api/valores/climatologicos/diarios/datos/fechaini/2016-05-01T00%3A00%3A00TC/fechafin/2016-08-01T00%3A00%3A00TC/estacion/3195
")
estado.trafico.zgz <- readLines("https://opendata.aemet.es/opendata/api/valores/climatologicos/diarios/datos/fechaini/2016-05-01T00%3A00%3A00TC/fechafin/2016-08-01T00%3A00%3A00TC/estacion/3195")
install.packages("jsonlite")
library(jsonlite)
meteo <- GET(curl -X GET --header 'Accept: application/javascript' --header 'api_key: eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJjLnBhbG9tb2dhcm9AZ21haWwuY29tIiwianRpIjoiMjJhMDgxOGItY2M3My00N2U4LWEwODQtZDE5YmI0MGIxNDY0IiwiaXNzIjoiQUVNRVQiLCJpYXQiOjE1MDg5MzQyNDIsInVzZXJJZCI6IjIyYTA4MThiLWNjNzMtNDdlOC1hMDg0LWQxOWJiNDBiMTQ2NCIsInJvbGUiOiIifQ.q3G3FKx272DztYbDtHoeo6-mOuY5n3wYhlD7DDBSuKc' 'https://opendata.aemet.es/opendata/api/valores/climatologicos/diarios/datos/fechaini/2016-05-01T00%3A00%3A00TC/fechafin/2016-08-01T00%3A00%3A00TC/estacion/3195')
meteo <- GET('https://opendata.aemet.es/opendata/api/valores/climatologicos/diarios/datos/fechaini/2016-05-01T00%3A00%3A00TC/fechafin/2016-08-01T00%3A00%3A00TC/estacion/3195')
library(httr)
meteo <- GET('https://opendata.aemet.es/opendata/api/valores/climatologicos/diarios/datos/fechaini/2016-05-01T00%3A00%3A00TC/fechafin/2016-08-01T00%3A00%3A00TC/estacion/3195')
install.packages("curlconverter")
meteo <- GET("https://opendata.aemet.es/opendata/sh/526201d3")
meteo <- GET("https://opendata.aemet.es/opendata/sh/526201d3", query = list(api_key = "eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJjLnBhbG9tb2dhcm9AZ21haWwuY29tIiwianRpIjoiMjJhMDgxOGItY2M3My00N2U4LWEwODQtZDE5YmI0MGIxNDY0IiwiaXNzIjoiQUVNRVQiLCJpYXQiOjE1MDg5MzQyNDIsInVzZXJJZCI6IjIyYTA4MThiLWNjNzMtNDdlOC1hMDg0LWQxOWJiNDBiMTQ2NCIsInJvbGUiOiIifQ.q3G3FKx272DztYbDtHoeo6-mOuY5n3wYhlD7DDBSuKc"))
meteo <- GET("https://opendata.aemet.es/opendata/sh/526201d3", api_key = "eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJjLnBhbG9tb2dhcm9AZ21haWwuY29tIiwianRpIjoiMjJhMDgxOGItY2M3My00N2U4LWEwODQtZDE5YmI0MGIxNDY0IiwiaXNzIjoiQUVNRVQiLCJpYXQiOjE1MDg5MzQyNDIsInVzZXJJZCI6IjIyYTA4MThiLWNjNzMtNDdlOC1hMDg0LWQxOWJiNDBiMTQ2NCIsInJvbGUiOiIifQ.q3G3FKx272DztYbDtHoeo6-mOuY5n3wYhlD7DDBSuKc")
meteo <- GET("https://opendata.aemet.es/opendata/sh/526201d3", "eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJjLnBhbG9tb2dhcm9AZ21haWwuY29tIiwianRpIjoiMjJhMDgxOGItY2M3My00N2U4LWEwODQtZDE5YmI0MGIxNDY0IiwiaXNzIjoiQUVNRVQiLCJpYXQiOjE1MDg5MzQyNDIsInVzZXJJZCI6IjIyYTA4MThiLWNjNzMtNDdlOC1hMDg0LWQxOWJiNDBiMTQ2NCIsInJvbGUiOiIifQ.q3G3FKx272DztYbDtHoeo6-mOuY5n3wYhlD7DDBSuKc")
library(rvest)
meeo <- read_html("https://opendata.aemet.es/opendata/sh/526201d3")
install.packages("sp")
install.packages("RJSONIO")
library(jsonlite)
library(httr)
library(curl)
library(sp)
library(RJSONIO)
aemet_stations <- function(apikey) {
h <- new_handle()
handle_setheaders(h, `Cache-Control` = "no-cache", api_key = apikey)
handle_setopt(h, ssl_verifypeer = FALSE)
handle_setopt(h, customrequest = "GET")
url <- paste("https://opendata.aemet.es/opendata/api/valores/climatologicos/diarios/datos/fechaini/2016-05-01T00%3A00%3A00TC/fechafin/2016-08-01T00%3A00%3A00TC/estacion/3195")
md_raw <- curl_fetch_memory(url, h)
urldatos <- strsplit(rawToChar(md_raw$content), split = "\"")[[1]][10]
url2 <- httr::handle(urldatos)
set_config(config(ssl_verifypeer = 0L))
q1 <- httr::GET(handle = url2)
dataQ1 <- httr::content(q1, type = "text/plain", encoding = "ISO-8859-15")
listQ1 <- fromJSON(dataQ1)
df <- data.frame(t(sapply(listQ1, function(e) e)))
station_id <- df[, "indicativo"]
longString <- as.character(df[, "longitud"])
latString <- as.character(df[, "latitud"])
deg <- as.numeric(substr(longString, 0, 2))
min <- as.numeric(substr(longString, 3,4))
sec <- as.numeric(substr(longString, 5,6))
x <- deg + min/60 + sec/3600
x <- ifelse(substr(longString, 7, 8) == "W", x, -x)
deg <- as.numeric(substr(latString, 0, 2))
min <- as.numeric(substr(latString, 3,4))
sec <- as.numeric(substr(latString, 5,6))
y <- deg + min/60 + sec/3600
points <- SpatialPoints(coords = cbind(-x, y), CRS("+proj=longlat"))
colnames(points@coords) <- c("longitude", "latitude")
dfout <- data.frame(station_id = df[, "indicativo"],
station = df[, "nombre"],
province = df[, "provincia"],
elevation = as.numeric(df[, "altitud"]))
finaldf <- SpatialPointsDataFrame(points, dfout)
return(finaldf)
}
retiro <- aemet_stations(eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJjLnBhbG9tb2dhcm9AZ21haWwuY29tIiwianRpIjoiMjJhMDgxOGItY2M3My00N2U4LWEwODQtZDE5YmI0MGIxNDY0IiwiaXNzIjoiQUVNRVQiLCJpYXQiOjE1MDg5MzQyNDIsInVzZXJJZCI6IjIyYTA4MThiLWNjNzMtNDdlOC1hMDg0LWQxOWJiNDBiMTQ2NCIsInJvbGUiOiIifQ.q3G3FKx272DztYbDtHoeo6-mOuY5n3wYhlD7DDBSuKc)
API_KEY <- eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJjLnBhbG9tb2dhcm9AZ21haWwuY29tIiwianRpIjoiMjJhMDgxOGItY2M3My00N2U4LWEwODQtZDE5YmI0MGIxNDY0IiwiaXNzIjoiQUVNRVQiLCJpYXQiOjE1MDg5MzQyNDIsInVzZXJJZCI6IjIyYTA4MThiLWNjNzMtNDdlOC1hMDg0LWQxOWJiNDBiMTQ2NCIsInJvbGUiOiIifQ.q3G3FKx272DztYbDtHoeo6-mOuY5n3wYhlD7DDBSuKc
retiro <- aemet_stations(API_KEY)
API_KEY <- "eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJjLnBhbG9tb2dhcm9AZ21haWwuY29tIiwianRpIjoiMjJhMDgxOGItY2M3My00N2U4LWEwODQtZDE5YmI0MGIxNDY0IiwiaXNzIjoiQUVNRVQiLCJpYXQiOjE1MDg5MzQyNDIsInVzZXJJZCI6IjIyYTA4MThiLWNjNzMtNDdlOC1hMDg0LWQxOWJiNDBiMTQ2NCIsInJvbGUiOiIifQ.q3G3FKx272DztYbDtHoeo6-mOuY5n3wYhlD7DDBSuKc"
retiro <- aemet_stations(API_KEY)
retiro <- aemet_stations(API_KEY)
aemet_stations <- function(apikey) {
h <- new_handle()
handle_setheaders(h, `Cache-Control` = "no-cache", api_key = apikey)
handle_setopt(h, ssl_verifypeer = FALSE)
handle_setopt(h, customrequest = "GET")
url <- paste("https://opendata.aemet.es/opendata/api/valores/climatologicos/diarios/datos/fechaini/2016-05-01T00%3A00%3A00TC/fechafin/2016-08-01T00%3A00%3A00TC/estacion/3195")
md_raw <- curl_fetch_memory(url, h)
urldatos <- strsplit(rawToChar(md_raw$content), split = "\"")[[1]][10]
url2 <- httr::handle(urldatos)
set_config(config(ssl_verifypeer = 0L))
q1 <- httr::GET(handle = url2)
dataQ1 <- httr::content(q1, type = "text/plain", encoding = "ISO-8859-15")
}
retiro <- aemet_stations(API_KEY)
aemet_stations <- function(apikey) {
h <- new_handle()
handle_setheaders(h, `Cache-Control` = "no-cache", api_key = apikey)
handle_setopt(h, ssl_verifypeer = FALSE)
handle_setopt(h, customrequest = "GET")
url <- paste("https://opendata.aemet.es/opendata/api/valores/climatologicos/diarios/datos/fechaini/2017-05-01T00:00:00UTC/fechafin/2017-05-31T23:59:59UTC/estacion/3195/?api_key=eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJjLnBhbG9tb2dhcm9AZ21haWwuY29tIiwianRpIjoiMjJhMDgxOGItY2M3My00N2U4LWEwODQtZDE5YmI0MGIxNDY0IiwiaXNzIjoiQUVNRVQiLCJpYXQiOjE1MDg5MzQyNDIsInVzZXJJZCI6IjIyYTA4MThiLWNjNzMtNDdlOC1hMDg0LWQxOWJiNDBiMTQ2NCIsInJvbGUiOiIifQ.q3G3FKx272DztYbDtHoeo6-mOuY5n3wYhlD7DDBSuKc")
md_raw <- curl_fetch_memory(url, h)
urldatos <- strsplit(rawToChar(md_raw$content), split = "\"")[[1]][10]
url2 <- httr::handle(urldatos)
set_config(config(ssl_verifypeer = 0L))
q1 <- httr::GET(handle = url2)
dataQ1 <- httr::content(q1, type = "text/plain", encoding = "ISO-8859-15")
}
retiro <- aemet_stations(API_KEY)
retiro02 <- fromJSON(retiro)
retiro02 <- fromJSON(retiro) %>% as.data.frame
library(dplyr)
retiro02 <- fromJSON(retiro) %>% as.data.frame
retiro02 <- fromJSON(retiro) %>% as.data.frame()
retiro02 <- as.data.frame(retiro02)
retiro02 <- as.data.frame.table(retiro02)
estado.trafico.zgz <- readLines("http://www.zaragoza.es/trafico/estado/estado.json")
estado.trafico.zgz <- fromJSON(estado.trafico.zgz)
tramos.zgz <- readLines("http://www.zaragoza.es/trafico/estado/tramoswgs84.json")
tramos.zgz <- fromJSON(tramos.zgz)
estados <- strsplit(estado.trafico.zgz$estados, "")[[1]]
tramo.a.df <- function(tramo){
latlon <- do.call(rbind, lapply(tramo$points, as.data.frame))
latlon$id     <- tramo$id
latlon$name   <- tramo$name
latlon$estado <- estados[tramo$id]
latlon
}
tramos <- do.call(rbind, lapply(tramos.zgz[[1]], tramo.a.df))
plot(tramos$lon, tramos$lat, col = factor(tramos$estado))
estados <- strsplit(retiro02$fecha, "")[[1]]
estados <- strsplit(retiro02$names, "")[[1]]
rm(estado.trafico.zgz, tramos.zgz, tramo.a.df)
df <- data.frame(matrix(unlist(retiro), nrow=132, byrow=T))
View(df)
retiro02 <- fromJSON(retiro, flatten = TRUE)
df <- data.frame(matrix(unlist(retiro), nrow=132, byrow=T))
View(df)
retiro02 <- fromJSON(retiro, flatten = TRUE) %>% as.data.frame()
retiro02 <- fromJSON(retiro, flatten = TRUE) %>% as.data.frame
retiro02 <- fromJSON(retiro, flatten = TRUE) %>% as.data.frame(nrow = 30)
retiro02 <- fromJSON(retiro, flatten = TRUE) %>% slice(1:30) %>%  as.data.frame(nrow = 30)
retiro02 <- fromJSON(retiro, flatten = TRUE) %>% as.data.frame(nrow = 30) %>%  slice(1:30)
aemet_stations <- function(apikey) {
h <- new_handle()
handle_setheaders(h, `Cache-Control` = "no-cache", api_key = apikey)
handle_setopt(h, ssl_verifypeer = FALSE)
handle_setopt(h, customrequest = "GET")
url <- paste("https://opendata.aemet.es/opendata/sh/4a687bd7")
md_raw <- curl_fetch_memory(url, h)
urldatos <- strsplit(rawToChar(md_raw$content), split = "\"")[[1]][10]
url2 <- httr::handle(urldatos)
set_config(config(ssl_verifypeer = 0L))
q1 <- httr::GET(handle = url2)
dataQ1 <- httr::content(q1, type = "text/plain", encoding = "ISO-8859-15")
}
retiro03 <- aemet_stations(API_KEY)
setwd("~/Escritorio/m30/datasets")
library(CausalImpact)
library(dplyr)
library(lubridate)
# Periodo Mayo-Julio de 2017
# Datos de precipitaciones de la estación de Retiro (idema 3195) sacados de AEMET
mayo <- read.csv("mayo2017.csv")
junio <- read.csv('junio2017.csv')
julio <- read.csv('julio2017.csv')
precip <- rbind(mayo, junio)
precip <- rbind(precip, julio)
precip$Fecha <- ymd(precip$Fecha)
# Datos del tráfico de Calle30 del portal de datos abiertos del Ayuntamiento
trafico <- read.table('datosM30XMLclean.txt', header = TRUE,
sep = ',', stringsAsFactors = FALSE, blank.lines.skip = TRUE)
trafico$Fecha <- dmy(trafico$Fecha)
trafico02 <- trafico %>%
select(Fecha, UsuariosCalle30, velocidadMedia) %>%
filter(Fecha < '2017-08-01' & Fecha > '2017-04-30')
# Uno todos los datasets
todo <- inner_join(trafico02, precip, by = 'Fecha')
todo <- todo[order(as.Date(todo$Fecha, format='%yyyy-%mm-%dd')),]
#para que funcione el modelo, las filas deben estar ordenadas según la fecha
rownames(todo) <- 1:nrow(todo) # y numeradas conforme a ese orden para poder
#fijar un evento en concreto
## luego vi que se podia hacer con as.Date en pre y post.period
rm(mayo, junio, julio, trafico) #borro los innecesarios
# Modelo CausalImpact
# el 07-07-2017 se recogieron 19.3 mm³ (posición 67, fijamos el pre.period en 66)
pre.period66 <- c(1, 66)
post.period66 <- c(67, 91)
tmp <- todo
tmp$Fecha <- NULL
impact66 <- CausalImpact(tmp, pre.period66, post.period66)
plot(impact66)
# el 10-05-2017 se recogieron 15 mm³ (posición 10)
pre.period09 <- c(1, 09)
post.period09 <- c(10, 91)
impact09 <- CausalImpact(tmp, pre.period09, post.period09)
plot(impact09)
## no tiene conjunto de entrenamiento. No se ve la periodicidad
# el 17-05-2017 se recogieron 6.3 mm³ (posición 17)
pre.period16 <- c(1, 16)
post.period16 <- c(17, 91)
impact16 <- CausalImpact(tmp, pre.period16, post.period16)
plot(impact16)
library(readr)
X201705_DatosTrafico <- read_csv("~/Escritorio/m30/datasets/201705_DatosTrafico.csv")
View(X201705_DatosTrafico)
View(precip)
